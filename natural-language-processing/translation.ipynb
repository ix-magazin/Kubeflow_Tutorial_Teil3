{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50201f05-aee8-46c6-937b-3acb866a6e95",
   "metadata": {},
   "source": [
    "# NLP Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c4e4bc7f-74d0-4b44-b66d-8238f35644c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fe452930-025e-4f3e-b72f-020f21b2d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_MODEL_COMPONENT_URL = \"https://raw.githubusercontent.com/lehrig/kubeflow-ppc64le-components/main/model-building/upload-model/component.yaml\"\n",
    "MODEL_NAME = \"question-answering\"\n",
    "MINIO_URL = \"minio-service.kubeflow:9000\"\n",
    "MINIO_USER = \"minio\"\n",
    "MINIO_PASS = \"minio123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f7cd101f-5646-4cab-922b-9cbd2ed24915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir: comp.OutputPath(str)):\n",
    "    from datasets import load_dataset\n",
    "    import os\n",
    "    squad = load_dataset(\"squad\")\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "    \n",
    "    # with open(dataset_dir + \"/dataset.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(squad, f)\n",
    "    squad.save_to_disk(dataset_dir)\n",
    "\n",
    "        \n",
    "load_dataset_comp = kfp.components.create_component_from_func(load_dataset, \"dataset.yaml\",\n",
    "                                                              \"quay.io/jeremie_ch/transformers-component:gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5bf7d3ea-1aa6-4711-8c22-25f2a4d787f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(dataset_dir: comp.InputPath(str)):\n",
    "    import pickle\n",
    "    from datasets.load import load_from_disk\n",
    "    \n",
    "    dataset = load_from_disk(dataset_dir)\n",
    "    \n",
    "    # with open(dataset_dir + \"/dataset.pkl\", \"rb\") as f:\n",
    "    #     dataset = pickle.load(f)\n",
    "    # dataset.describe()\n",
    "\n",
    "\n",
    "description_comp = kfp.components.create_component_from_func(description, \"description.yaml\",\n",
    "                                                             \"quay.io/jeremie_ch/transformers-component:gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b1bf3bf7-0fd9-4a77-812b-f69cb7db3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset_dir: comp.InputPath(str),\n",
    "               preprocess_dir: comp.OutputPath(str)):\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    from datasets.load import load_from_disk\n",
    "    from datasets import load_dataset\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    # with open(dataset_dir + \"/dataset.pkl\", \"rb\") as f:\n",
    "    #     squad = pickle.load(f)\n",
    "    squad = load_from_disk(dataset_dir)\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            max_length=384,\n",
    "            truncation=\"only_second\",\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "        answers = examples[\"answers\"]\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            answer = answers[i]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            # If the answer is not fully inside the context, label it (0, 0)\n",
    "            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Otherwise it's the start and end token positions\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "        inputs[\"start_positions\"] = start_positions\n",
    "        inputs[\"end_positions\"] = end_positions\n",
    "        return inputs\n",
    "\n",
    "    tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n",
    "    \n",
    "    if not os.path.exists(preprocess_dir):\n",
    "        os.makedirs(preprocess_dir)\n",
    "    \n",
    "    tokenized_squad.save_to_disk(preprocess_dir)\n",
    "\n",
    "\n",
    "preprocess_comp = kfp.components.create_component_from_func(preprocess, \"preprocess.yaml\",\n",
    "                                                            \"quay.io/jeremie_ch/transformers-component:gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "853d90d5-fbdb-44e9-9c97-e0bc7c33ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(preprocess_dir: comp.InputPath(str),\n",
    "          # model_dir: comp.OutputPath(str),\n",
    "          model_path: comp.OutputPath(str),\n",
    "          checkpoint_dir: comp.OutputPath(str)):\n",
    "\n",
    "    import os\n",
    "    from datasets import load_from_disk\n",
    "    import pickle\n",
    "    from transformers import AutoTokenizer, DefaultDataCollator, \\\n",
    "        AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "    \n",
    "    tokenized_squad = load_from_disk(preprocess_dir)\n",
    "    \n",
    "    data_collator = DefaultDataCollator()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=checkpoint_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1, #3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        # train_dataset=tokenized_squad[\"train\"],\n",
    "        train_dataset=tokenized_squad[\"train\"].select(range(1000)),\n",
    "        eval_dataset=tokenized_squad[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    model_dir = os.path.dirname(model_path)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    trainer.save_model(model_dir)\n",
    "\n",
    "\n",
    "train_comp = kfp.components.create_component_from_func(train, \"train.yaml\",\n",
    "                                                       \"quay.io/jeremie_ch/transformers-component:gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1fb7690d-5674-484a-9be3-a989db34df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_model_comp = comp.load_component_from_url(\n",
    "    UPLOAD_MODEL_COMPONENT_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c95ab00c-fac1-403b-bb7a-f730e400b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(dataset_dir: str, \n",
    "             preprocess_dir: str, \n",
    "             # model_dir: str,\n",
    "             checkpoint_dir: str,\n",
    "             model_path: str = \"/model_dir/training_args.bin\",\n",
    "             model_name: str = \"question-answering\",\n",
    "             minio_url: str = MINIO_URL,\n",
    "             minio_user: str = MINIO_USER,\n",
    "             minio_pass: str = MINIO_PASS):\n",
    "    load_dataset_task = load_dataset_comp()\n",
    "    description_task = description_comp(dataset_dir=load_dataset_task.output)\n",
    "    preproccess_task = preprocess_comp(dataset_dir=load_dataset_task.output)\n",
    "    train_task = train_comp(preprocess_dir=preproccess_task.output).set_gpu_limit(1)\n",
    "    upload_model_task = upload_model_comp(\n",
    "        train_task.outputs[\"model_path\"],\n",
    "        minio_url,\n",
    "        minio_user,\n",
    "        minio_pass,\n",
    "        model_name=model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a28597d6-f91b-4272-986c-77faa1158ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [263]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m NAMESPACE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjeremie-chheang-ibm-com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m client \u001b[38;5;241m=\u001b[39m kfp\u001b[38;5;241m.\u001b[39mClient()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_run_from_pipeline_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNAMESPACE\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/kfp/_client.py:995\u001b[0m, in \u001b[0;36mClient.create_run_from_pipeline_func\u001b[0;34m(self, pipeline_func, arguments, run_name, experiment_name, pipeline_conf, namespace, mode, launcher_image, pipeline_root, enable_caching, service_account)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m tmpdir:\n\u001b[1;32m    994\u001b[0m     pipeline_package_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 995\u001b[0m     \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompiler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlauncher_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlauncher_image\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpackage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_package_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpipeline_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_run_from_pipeline_package(\n\u001b[1;32m   1002\u001b[0m         pipeline_file\u001b[38;5;241m=\u001b[39mpipeline_package_path,\n\u001b[1;32m   1003\u001b[0m         arguments\u001b[38;5;241m=\u001b[39marguments,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m   1010\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/kfp/compiler/compiler.py:1175\u001b[0m, in \u001b[0;36mCompiler.compile\u001b[0;34m(self, pipeline_func, package_path, type_check, pipeline_conf)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1174\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mTYPE_CHECK \u001b[38;5;241m=\u001b[39m type_check\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_write_workflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpackage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mTYPE_CHECK \u001b[38;5;241m=\u001b[39m type_check_old_value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/kfp/compiler/compiler.py:1227\u001b[0m, in \u001b[0;36mCompiler._create_and_write_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf, package_path)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_and_write_workflow\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1219\u001b[0m                                pipeline_func: Callable,\n\u001b[1;32m   1220\u001b[0m                                pipeline_name: Text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1223\u001b[0m                                pipeline_conf: dsl\u001b[38;5;241m.\u001b[39mPipelineConf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1224\u001b[0m                                package_path: Text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;124;03m\"\"\"Compile the given pipeline function and dump it to specified file\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m    format.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m     workflow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpipeline_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpipeline_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_workflow(workflow, package_path)\n\u001b[1;32m   1231\u001b[0m     _validate_workflow(workflow)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/kfp/compiler/compiler.py:1005\u001b[0m, in \u001b[0;36mCompiler._create_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         args_list\u001b[38;5;241m.\u001b[39mappend(param)\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dsl\u001b[38;5;241m.\u001b[39mPipeline(pipeline_name) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[43mpipeline_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m pipeline_conf \u001b[38;5;241m=\u001b[39m pipeline_conf \u001b[38;5;129;01mor\u001b[39;00m dsl_pipeline\u001b[38;5;241m.\u001b[39mconf  \u001b[38;5;66;03m# Configuration passed to the compiler is overriding. Unfortunately, it's not trivial to detect whether the dsl_pipeline.conf was ever modified.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_exit_handler(dsl_pipeline)\n",
      "Input \u001b[0;32mIn [253]\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(dataset_dir, preprocess_dir, checkpoint_dir, model_path, model_name, minio_url, minio_user, minio_pass)\u001b[0m\n\u001b[1;32m     12\u001b[0m preproccess_task \u001b[38;5;241m=\u001b[39m preprocess_comp(dataset_dir\u001b[38;5;241m=\u001b[39mload_dataset_task\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m     13\u001b[0m train_task \u001b[38;5;241m=\u001b[39m train_comp(preprocess_dir\u001b[38;5;241m=\u001b[39mpreproccess_task\u001b[38;5;241m.\u001b[39moutput)\u001b[38;5;241m.\u001b[39mset_gpu_limit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m upload_model_task \u001b[38;5;241m=\u001b[39m upload_model_comp(\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mtrain_task\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     16\u001b[0m     minio_url,\n\u001b[1;32m     17\u001b[0m     minio_user,\n\u001b[1;32m     18\u001b[0m     minio_pass,\n\u001b[1;32m     19\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_path'"
     ]
    }
   ],
   "source": [
    "arguments = {\"dataset_dir\": \"/dataset_dir\", \n",
    "             \"preprocess_dir\": \"/preprocess_dir\",\n",
    "             # \"model_dir\": \"/model_dir\", \n",
    "             \"model_name\": MODEL_NAME,\n",
    "             \"checkpoint_dir\": \"/checkpoint_dir\",\n",
    "             \"model_path\": \"/model_dir/training_args.bin\",\n",
    "             \"minio_url\": MINIO_URL,\n",
    "             \"minio_user\": MINIO_USER,\n",
    "             \"minio_pass\": MINIO_PASS}\n",
    "\n",
    "NAMESPACE = \"jeremie-chheang-ibm-com\"\n",
    "client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline,\n",
    "    arguments=arguments,\n",
    "    namespace=NAMESPACE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6681a89d-bdbe-4a2c-a534-4792200db57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/model_dir'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c734a-ef47-4312-9af2-efcbef4ac243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
