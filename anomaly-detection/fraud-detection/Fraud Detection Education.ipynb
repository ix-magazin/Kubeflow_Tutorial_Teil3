{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f59d678-011b-4ac0-bb14-2fac70039a04",
   "metadata": {},
   "source": [
    "# Fraud Detection\n",
    "\n",
    "Classify transactions as fraud or non-fraud using an LSTM-based neural network. Data coming from https://github.com/IBM/TabFormer/tree/main/data/credit_card\n",
    "\n",
    "## Authors\n",
    "\n",
    "Natalie Jann [natalie.jann@ibm.com](mailto:natalie.jann@ibm.com)\n",
    "\n",
    "Sebastian Lehrig [sebastian.lehrig1@ibm.com](mailto:sebastian.lehrig1@ibm.com)\n",
    "\n",
    "Marvin Giessing [MARVING@de.ibm.com](mailto:MARVING@de.ibm.com)\n",
    "\n",
    "## License\n",
    "\n",
    "Apache-2.0 License\n",
    "\n",
    "## 0.) Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6f6919-b7f0-45ff-ab6b-c42c4fea458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kfp\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    OutputPath\n",
    ")\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import (\n",
    "    PipelineConf,\n",
    "    data_passing_methods\n",
    ")\n",
    "from kubernetes.client.models import (\n",
    "    V1Volume,\n",
    "    V1PersistentVolumeClaimVolumeSource\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b80193b-7c64-42eb-9f34-7cf9d3e75b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blackboard': 'artefacts',\n",
       " 'model_name': 'fraud-detection',\n",
       " 'cluster_configuration_secret': ''}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\"\n",
    "COMPONENT_CATALOG_GIT = \"https://github.com/lehrig/kubeflow-ppc64le-components.git\"\n",
    "COMPONENT_CATALOG_RELEASE = \"main\"\n",
    "\n",
    "CONVERT_MODEL_TO_ONNX_COMPONENT = f\"{COMPONENT_CATALOG_FOLDER}/model-building/convert-to-onnx/component.yaml\"\n",
    "UPLOAD_MODEL_COMPONENT = f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    "DEPLOY_MODEL_WITH_KSERVE_COMPONENT = f\"{COMPONENT_CATALOG_FOLDER}/model-deployment/deploy-model-with-kserve/component.yaml\"\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "ARGUMENTS = {\n",
    "    'blackboard': 'artefacts',\n",
    "    'model_name': \"fraud-detection\",\n",
    "    'cluster_configuration_secret': os.getenv('CLUSTER_CONFIGURATION_SECRET', default=''),\n",
    "}\n",
    "MODEL_NAME = ARGUMENTS[\"model_name\"]\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "\n",
    "ARGUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa41798-cc31-4679-922f-3a08344aa026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/home/jovyan/components' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch $COMPONENT_CATALOG_RELEASE $COMPONENT_CATALOG_GIT $COMPONENT_CATALOG_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536aff6-9ff7-4ddd-9cfe-f633d101ea84",
   "metadata": {},
   "source": [
    "## 1.) Component Definition: Dataset Loading, Rebalancing & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dadbaf5-671c-4dbe-b6f9-5008e5ef0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function wrapping the code below to load the dataset \n",
    "# the function signature should have the following parameters: \n",
    "# columns: list, test_dataset_dir: OutputPath(str), train_dataset_dir: OutputPath(str)\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trino.dbapi import Connection\n",
    "\n",
    "def save_to_dir(x, y, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    np.savez(os.path.join(directory, 'data'), x=x, y=y)\n",
    "\n",
    "with Connection(\n",
    "    host='trino.trino',\n",
    "    port='8080',\n",
    "    user=\"anybody\",\n",
    "    catalog='postgresql',\n",
    "    schema='public',\n",
    ") as conn:\n",
    "    link = conn.cursor()\n",
    "    # update the query below to retrieve only 999980 transactions\n",
    "    link.execute('SELECT * FROM transactions')\n",
    "    tdf = pd.DataFrame(link.fetchall())\n",
    "tdf.columns = columns\n",
    "print(f'Retrieved {len(tdf)} rows')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tdf.drop(['Is Fraud?'], axis=1),\n",
    "    tdf['Is Fraud?'],\n",
    "    test_size=0.2,\n",
    "    random_state=37\n",
    ")\n",
    "\n",
    "min_ind = math.floor(len(X_train)/128)\n",
    "X_train, y_train = X_train[-min_ind*128:], y_train[-min_ind*128:]\n",
    "min_ind = math.floor(len(X_test)/128)\n",
    "X_test, y_test = X_test[-min_ind*128:].to_numpy(), y_test[-min_ind*128:].to_numpy()\n",
    "\n",
    "over_sampler = RandomOverSampler(random_state=37, sampling_strategy=0.5)\n",
    "train_input, train_target = over_sampler.fit_resample(X_train, y_train)\n",
    "print(sum(train_target==0), 'negative &', sum(train_target==1), 'positive samples (after upsampling)')\n",
    "\n",
    "train_input = train_input.to_numpy()\n",
    "train_target = train_target.to_numpy().reshape(len(train_target), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "save_to_dir(X_train, y_train, train_dataset_dir)\n",
    "save_to_dir(X_test, y_test, test_dataset_dir)\n",
    "\n",
    "print(f\"Pre-processed train dataset saved. Contents of '{train_dataset_dir}':\")\n",
    "print(os.listdir(train_dataset_dir))\n",
    "print(f\"Pre-processed test dataset saved. Contents of '{test_dataset_dir}':\")\n",
    "print(os.listdir(test_dataset_dir))\n",
    "\n",
    "## define a component from the function above \n",
    "# use the BASE_IMAGE\n",
    "# the component should install the package 'imbalanced-learn'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a6175-476a-41fd-aba8-7f47a9faf1a2",
   "metadata": {},
   "source": [
    "## 2.) Component Definition: Model Training, Evaluation & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f41327-d0e7-4cf2-b4f9-8587f7a464f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function wrapping the code below to train the model\n",
    "# the function signature should have the following parameters:\n",
    "# model_dir: OutputPath(str), test_dataset_dir: InputPath(str), train_dataset_dir: InputPath(str), seq_len: int = 7\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.metrics import TruePositives, FalsePositives, FalseNegatives, TrueNegatives\n",
    "\n",
    "metrics = ['accuracy',\n",
    "           TruePositives(name='tp'),\n",
    "           FalsePositives(name='fp'),\n",
    "           FalseNegatives(name='fn'),\n",
    "           TrueNegatives(name='tn')\n",
    "           ]\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = np.load(os.path.join(path, 'data.npz'))\n",
    "    x, y = data['x'], data['y']\n",
    "    dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x, y, sequence_length=seq_len, batch_size=128)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = load_dataset(train_dataset_dir)\n",
    "test_dataset = load_dataset(test_dataset_dir)\n",
    "\n",
    "for batch in train_dataset.take(1):\n",
    "    input_d, targets = batch\n",
    "print(\"Input shape:\", input_d.numpy().shape, \"Target shape:\", targets.numpy().shape)\n",
    "\n",
    "input_shape = (input_d.shape[1], input_d.shape[2])\n",
    "inputs = Input(shape=input_shape)\n",
    "lstm_in = LSTM(200, batch_size=7, return_sequences=True)(inputs)\n",
    "lstm_out = LSTM(200, batch_size=7)(lstm_in)\n",
    "outputs = Dense(1, activation='sigmoid')(lstm_out)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=metrics)\n",
    "print(model.summary())\n",
    "\n",
    "# make the number of epochs & steps per epoch dynamic by adding it to the function signature\n",
    "model.fit(train_dataset, epochs=5, steps_per_epoch=500, verbose=3)\n",
    "\n",
    "results = model.evaluate(test_dataset)\n",
    "print('Evaluation Loss, Accuracy, TP, FP, FN, TN:', results)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model.save(model_dir)\n",
    "\n",
    "\n",
    "## define a component from the function above \n",
    "# use the BASE_IMAGE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4fbdaf7-4a4a-448f-b3d4-17b8abcc3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_model_to_onnx_comp = kfp.components.load_component_from_file(\n",
    "    CONVERT_MODEL_TO_ONNX_COMPONENT\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(\n",
    "    UPLOAD_MODEL_COMPONENT\n",
    ")\n",
    "\n",
    "## apply the schema of defining components from a file shown right above \n",
    "# define a component for the model deployment using DEPLOY_MODEL_WITH_KSERVE_COMPONENT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c3a7c-887d-43a8-aca7-ff877afd1eb4",
   "metadata": {},
   "source": [
    "## 3.) Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8da0800-4e73-4da6-b09b-197fd51654d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Fraud detection pipeline',\n",
    "  description='An example pipeline that tries to predict fraudulent credit card transactions'\n",
    ")\n",
    "def fraud_pipeline(\n",
    "    blackboard: str,\n",
    "    model_name: str,\n",
    "    ## add the number of epochs to train as a pipeline parameter here \n",
    "    # don't forget to update the ARGUMENTS at the beginning of the notebook, too\n",
    "    cluster_configuration_secret: str\n",
    "):\n",
    "\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name='Create Artefacts Blackboard',\n",
    "        resource_name=blackboard,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True\n",
    "    )\n",
    "\n",
    "    load_dataset_task = load_dataset_comp(\n",
    "        columns=columns\n",
    "    )\n",
    "    load_dataset_task.after(create_blackboard)\n",
    "\n",
    "    train_model_task = train_model_comp(\n",
    "        ## set the parameter 'epochs' and 'steps_per_epoch' from the train model function here\n",
    "        # use other than the default values\n",
    "\n",
    "        test_dataset_dir=load_dataset_task.outputs['test_dataset_dir'],\n",
    "        train_dataset_dir=load_dataset_task.outputs['train_dataset_dir']\n",
    "    )\n",
    "    convert_model_to_onnx_task = convert_model_to_onnx_comp(\n",
    "        train_model_task.outputs['model_dir']\n",
    "    )\n",
    "\n",
    "    upload_model_task = upload_model_comp(\n",
    "        convert_model_to_onnx_task.outputs['onnx_model_dir'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    ## add a new task below to run the model deployment component\n",
    "    # set model_name=model_name\n",
    "\n",
    "    deploy_model_with_kserve_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f5cb70a-1e40-438f-918f-85dc903909ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/6f189cc3-98f2-4594-bfb2-c9688a1ec5c3\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/61a00ca6-bed5-4090-b6c0-c7bf0fef770f\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=61a00ca6-bed5-4090-b6c0-c7bf0fef770f)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def disable_cache_transformer(op):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\")\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf = PipelineConf()\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "pipeline_conf.data_passing_method = data_passing_methods.KubernetesVolume(\n",
    "    volume=V1Volume(\n",
    "        name=ARGUMENTS[\"blackboard\"],\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "            \"{{workflow.name}}-%s\" % ARGUMENTS[\"blackboard\"]\n",
    "        ),\n",
    "    ),\n",
    "    path_prefix=f'{ARGUMENTS[\"blackboard\"]}/',\n",
    ")\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    fraud_pipeline,\n",
    "    arguments=ARGUMENTS,\n",
    "    namespace=NAMESPACE,\n",
    "    pipeline_conf=pipeline_conf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778778e2-cc0a-4f16-8300-2533a4c3e379",
   "metadata": {},
   "source": [
    "## 4.) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28e983-a0fe-4e36-9397-ca2b849fee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "HOST = f'{MODEL_NAME}-predictor-default.{NAMESPACE}'\n",
    "HEADERS = {'Host': HOST}\n",
    "MODEL_ENDPOINT = f'http://{MODEL_NAME}-predictor-default/v2/models/{MODEL_NAME}'\n",
    "\n",
    "res = requests.get(MODEL_ENDPOINT, headers=HEADERS)\n",
    "response = json.loads(res.text)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af216a33-cb22-47dc-9349-a229d2ee8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from requests import post\n",
    "from tensorflow import keras\n",
    "\n",
    "with Connection(\n",
    "    host='trino.trino',\n",
    "    port='8080',\n",
    "    user=\"anybody\",\n",
    "    catalog='postgresql',\n",
    "    schema='public',\n",
    ") as conn:\n",
    "    link = conn.cursor()\n",
    "    link.execute('SELECT * FROM transactions OFFSET 999980')\n",
    "    vdf = pd.DataFrame(link.fetchall())\n",
    "vdf.columns = columns\n",
    "print(f'Retrieved {len(tdf)} rows')\n",
    "\n",
    "x, y = vdf.drop(['Is Fraud?'], axis=1).to_numpy(), vdf['Is Fraud?'].to_numpy().reshape(len(tdf), 1)\n",
    "dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x, y, sequence_length=7, batch_size=128)\n",
    "\n",
    "HOST = f'{MODEL_NAME}-predictor-default.{NAMESPACE}'\n",
    "HEADERS = {'Host': HOST}\n",
    "PREDICT_ENDPOINT = f'http://{MODEL_NAME}-predictor-default/v2/models/{MODEL_NAME}/infer'\n",
    "\n",
    "for batch in dataset.take(10):\n",
    "    input_d, output_d = batch[0], batch[1]\n",
    "    for in_x, out_y in zip(input_d, output_d):\n",
    "        payload = {\n",
    "          \"inputs\": [{\n",
    "              \"name\": \"input_1\",\n",
    "              \"shape\": [1, 7, 103],\n",
    "              \"datatype\": \"FP32\",\n",
    "              \"data\": in_x.numpy().tolist()\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        res = post(PREDICT_ENDPOINT, headers=HEADERS, data=json.dumps(payload))\n",
    "        response = json.loads(res.text)\n",
    "        print(\"Actual vs. Prediction\", out_y, round(response['outputs'][0]['data'][0], 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
