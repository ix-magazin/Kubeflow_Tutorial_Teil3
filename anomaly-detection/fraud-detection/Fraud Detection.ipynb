{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986f2554-fc02-4865-b1c8-c8dc2fcd445e",
   "metadata": {},
   "source": [
    "# Fraud Detection\n",
    "\n",
    "Classify transactions as fraud or non-fraud using an LSTM-based neural network. Data coming from https://github.com/IBM/TabFormer/tree/main/data/credit_card.\n",
    "\n",
    "## Authors\n",
    "\n",
    "Natalie Jann [natalie.jann@ibm.com](mailto:natalie.jann@ibm.com)\n",
    "\n",
    "Sebastian Lehrig [sebastian.lehrig1@ibm.com](mailto:sebastian.lehrig1@ibm.com)\n",
    "\n",
    "Marvin Giessing [MARVING@de.ibm.com](mailto:MARVING@de.ibm.com)\n",
    "\n",
    "## License\n",
    "\n",
    "Apache-2.0 License\n",
    "\n",
    "## 0.) Imports & Constants"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86682cb9-31c1-405e-bdd6-b237b642d6f1",
   "metadata": {},
   "source": [
    "conda install -y pydot libgfortran5 sklearn-pandas imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6f6919-b7f0-45ff-ab6b-c42c4fea458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client.models import V1Volume, V1PersistentVolumeClaimVolumeSource\n",
    "import os\n",
    "import requests\n",
    "from requests import post\n",
    "from tensorflow import keras\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5138b48-5058-4cb5-a0d8-364c043ec2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from https://ibm.ent.box.com/v/tabformer-data/file/770766751708 and upload here\n",
    "!tar -xvf transactions.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b80193b-7c64-42eb-9f34-7cf9d3e75b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blackboard': 'artefacts',\n",
       " 'model_name': 'fraud-detection',\n",
       " 'cluster_configuration_secret': '',\n",
       " 'training_gpus': '1'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\"\n",
    "COMPONENT_CATALOG_GIT = \"https://github.com/lehrig/kubeflow-ppc64le-components.git\"\n",
    "COMPONENT_CATALOG_RELEASE = \"main\"\n",
    "\n",
    "LOAD_DATAFRAME_VIA_TRINO_COMPONENT = f\"{COMPONENT_CATALOG_FOLDER}/data-collection/load-dataframe-via-trino/component.yaml\"\n",
    "MONITOR_TRAINING_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/monitor-training/component.yaml\"\n",
    ")\n",
    "TRAIN_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/train-model-job/component.yaml\"\n",
    ")\n",
    "CONVERT_MODEL_TO_ONNX_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/convert-to-onnx/component.yaml\"\n",
    ")\n",
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "DEPLOY_MODEL_WITH_KSERVE_COMPONENT = f\"{COMPONENT_CATALOG_FOLDER}/model-deployment/deploy-model-with-kserve/component.yaml\"\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "ARGUMENTS = {\n",
    "    \"blackboard\": \"artefacts\",\n",
    "    \"model_name\": \"fraud-detection\",\n",
    "    \"cluster_configuration_secret\": os.getenv(\n",
    "        \"CLUSTER_CONFIGURATION_SECRET\", default=\"\"\n",
    "    ),\n",
    "    \"training_gpus\": os.getenv(\"TRAINING_GPUS\", default=\"1\"),\n",
    "}\n",
    "MODEL_NAME = ARGUMENTS[\"model_name\"]\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "\n",
    "ARGUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa41798-cc31-4679-922f-3a08344aa026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/home/jovyan/components'...\n",
      "remote: Enumerating objects: 617, done.\u001b[K\n",
      "remote: Counting objects: 100% (271/271), done.\u001b[K\n",
      "remote: Compressing objects: 100% (152/152), done.\u001b[K\n",
      "remote: Total 617 (delta 164), reused 208 (delta 115), pack-reused 346\u001b[K\n",
      "Receiving objects: 100% (617/617), 217.87 KiB | 4.95 MiB/s, done.\n",
      "Resolving deltas: 100% (322/322), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch $COMPONENT_CATALOG_RELEASE $COMPONENT_CATALOG_GIT $COMPONENT_CATALOG_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536aff6-9ff7-4ddd-9cfe-f633d101ea84",
   "metadata": {},
   "source": [
    "## 1.) Component Definition: Dataset Loading, Rebalancing & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fcf3412-7b06-4a85-90c2-957d4722ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataframe_via_trino_comp = kfp.components.load_component_from_file(\n",
    "    LOAD_DATAFRAME_VIA_TRINO_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9dadbaf5-671c-4dbe-b6f9-5008e5ef0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(\n",
    "    dataframe: InputPath(str),\n",
    "    test_dataset_dir: OutputPath(str),\n",
    "    train_dataset_dir: OutputPath(str),\n",
    "):\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    def save_to_dir(x, y, directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        np.savez(os.path.join(directory, \"data\"), x=x, y=y)\n",
    "\n",
    "    def split_dataset(n, df):\n",
    "        test = df.iloc[:n, :]\n",
    "        train = df.iloc[n:, :]\n",
    "        return test, train\n",
    "\n",
    "    def merge_splits(frauds, non_frauds):\n",
    "        print(\"Ratio fraud/non-fraud:\", len(frauds) / len(non_frauds))\n",
    "        df = pd.concat([frauds, non_frauds])\n",
    "        df.sort_values(\"year_month_day_time\", inplace=True)\n",
    "\n",
    "        x, y = df.drop([\"is fraud\"], axis=1), df[\"is fraud\"]\n",
    "        min_ind = math.floor(len(x) / 128)\n",
    "        x, y = x[-min_ind * 128 :], y[-min_ind * 128 :]\n",
    "        return x, y\n",
    "\n",
    "    dataset = pd.read_feather(dataframe)\n",
    "    dataset = dataset.sample(frac=1)  # shuffle randomly\n",
    "    frauds = dataset[dataset[\"is fraud\"] == 1]\n",
    "    non_frauds = dataset[dataset[\"is fraud\"] == 0]\n",
    "    print(f\"{len(frauds)} Frauds and {len(non_frauds)} Non-Frauds.\")\n",
    "\n",
    "    n_frauds = int(0.001 * len(dataset))\n",
    "    n_non_frauds = int(len(dataset) * 0.2 - n_frauds)\n",
    "\n",
    "    test_frauds, train_frauds = split_dataset(n_frauds, frauds)\n",
    "    test_non_frauds, train_non_frauds = split_dataset(n_non_frauds, non_frauds)\n",
    "    x_train, y_train = merge_splits(train_frauds, train_non_frauds)\n",
    "    x_test, y_test = merge_splits(test_frauds, test_non_frauds)\n",
    "\n",
    "    over_sampler = RandomOverSampler(random_state=37, sampling_strategy=0.1)\n",
    "    train_input, train_target = over_sampler.fit_resample(x_train, y_train)\n",
    "    # train_input, train_target = x_train, y_train # use this if you don't want to oversample\n",
    "    print(\n",
    "        sum(train_target == 0),\n",
    "        \"negative &\",\n",
    "        sum(train_target == 1),\n",
    "        \"positive training samples (after upsampling)\",\n",
    "    )\n",
    "    print(\n",
    "        sum(y_test == 0),\n",
    "        \"negative &\",\n",
    "        sum(y_test == 1),\n",
    "        \"positive test samples\",\n",
    "    )\n",
    "    train = pd.concat([pd.DataFrame(train_target), pd.DataFrame(train_input)], axis=1)\n",
    "    train.columns = dataset.columns\n",
    "    train.sort_values(\"year_month_day_time\", inplace=True)\n",
    "    train_input, train_target = train.drop([\"is fraud\"], axis=1), train[\"is fraud\"]\n",
    "\n",
    "    train_target = train_target.to_numpy().reshape(len(train_target), 1)\n",
    "    y_test = y_test.to_numpy().reshape(len(y_test), 1)\n",
    "\n",
    "    save_to_dir(train_input.to_numpy(), train_target, train_dataset_dir)\n",
    "    save_to_dir(x_test.to_numpy(), y_test, test_dataset_dir)\n",
    "\n",
    "    print(f\"Pre-processed train dataset saved. Contents of '{train_dataset_dir}':\")\n",
    "    print(os.listdir(train_dataset_dir))\n",
    "    print(f\"Pre-processed test dataset saved. Contents of '{test_dataset_dir}':\")\n",
    "    print(os.listdir(test_dataset_dir))\n",
    "\n",
    "\n",
    "preprocess_dataset_comp = kfp.components.create_component_from_func(\n",
    "    func=preprocess_dataset,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"imbalanced-learn\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a6175-476a-41fd-aba8-7f47a9faf1a2",
   "metadata": {},
   "source": [
    "## 2.) Component Definition: Model Training, Evaluation & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "396c6c2f-999a-490c-97c3-c95a56cd5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_training_comp = kfp.components.load_component_from_file(\n",
    "    MONITOR_TRAINING_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "02f41327-d0e7-4cf2-b4f9-8587f7a464f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model_dir: OutputPath(str),\n",
    "    test_dataset_dir: InputPath(str),\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    epochs: int = 10,\n",
    "    seq_len: int = 7,\n",
    "):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.callbacks import (\n",
    "        EarlyStopping,\n",
    "        ModelCheckpoint,\n",
    "        ReduceLROnPlateau,\n",
    "        TensorBoard,\n",
    "    )\n",
    "    from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "    from tensorflow.keras.metrics import (\n",
    "        TruePositives,\n",
    "        FalsePositives,\n",
    "        FalseNegatives,\n",
    "        TrueNegatives,\n",
    "    )\n",
    "\n",
    "    def load_dataset(path):\n",
    "        data = np.load(os.path.join(path, \"data.npz\"))\n",
    "        x, y = data[\"x\"], data[\"y\"]\n",
    "        dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            x, y, sequence_length=seq_len, batch_size=128\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    train_dataset = load_dataset(train_dataset_dir)\n",
    "    test_dataset = load_dataset(test_dataset_dir)\n",
    "\n",
    "    for batch in train_dataset.take(1):\n",
    "        input_d, targets = batch\n",
    "    print(\"Input shape:\", input_d.numpy().shape, \"Target shape:\", targets.numpy().shape)\n",
    "\n",
    "    input_shape = (input_d.shape[1], input_d.shape[2])\n",
    "    inputs = Input(shape=input_shape)\n",
    "    lstm_in = LSTM(100, batch_size=7, return_sequences=True)(inputs)\n",
    "    lstm_out = LSTM(100, batch_size=7)(lstm_in)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(lstm_out)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        TruePositives(name=\"tp\"),\n",
    "        FalsePositives(name=\"fp\"),\n",
    "        FalseNegatives(name=\"fn\"),\n",
    "        TrueNegatives(name=\"tn\"),\n",
    "    ]\n",
    "    # loss = keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=metrics\n",
    "    )\n",
    "    print(model.summary())\n",
    "\n",
    "    print(\"Initializing training callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"loss\", patience=20, verbose=0, mode=\"min\"),\n",
    "        ModelCheckpoint(\n",
    "            f\"{model_dir}/best_model.keras\",\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.1,\n",
    "            patience=7,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=os.environ[\"TENSORBOARD_S3_ADDRESS\"],\n",
    "            histogram_freq=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        verbose=3,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    results = model.evaluate(test_dataset)\n",
    "    print(\"Evaluation Loss, Accuracy, TP, FP, FN, TN:\", results)\n",
    "    TP, FP, FN, TN = results[2:]\n",
    "    if TP != 0:\n",
    "        PR = TP / (FP + TP)\n",
    "        RE = TP / (FN + TP)\n",
    "        print(\"F1 Measure:\", 2 * (PR * RE / (PR + RE)))\n",
    "\n",
    "    model.save(model_dir)\n",
    "\n",
    "\n",
    "train_specification = kfp.components.func_to_component_text(func=train_model)\n",
    "train_model_comp = kfp.components.load_component_from_file(TRAIN_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b4fbdaf7-4a4a-448f-b3d4-17b8abcc3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_model_to_onnx_comp = kfp.components.load_component_from_file(\n",
    "    CONVERT_MODEL_TO_ONNX_COMPONENT\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)\n",
    "\n",
    "deploy_model_with_kserve_comp = kfp.components.load_component_from_file(\n",
    "    DEPLOY_MODEL_WITH_KSERVE_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c3a7c-887d-43a8-aca7-ff877afd1eb4",
   "metadata": {},
   "source": [
    "## 3.) Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e8da0800-4e73-4da6-b09b-197fd51654d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Fraud detection\",\n",
    "    description=\"An example pipeline that tries to predict fraudulent credit card transactions\",\n",
    ")\n",
    "def fraud_pipeline(\n",
    "    blackboard: str,\n",
    "    model_name: str,\n",
    "    cluster_configuration_secret: str,\n",
    "    training_gpus: int,\n",
    "):\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=blackboard,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    load_dataframe_via_trino_task = load_dataframe_via_trino_comp(\n",
    "        query=\"SELECT * FROM postgresql.public.transactions LIMIT 1247592\",\n",
    "        columns_query=\"SHOW COLUMNS FROM postgresql.public.transactions\",\n",
    "    )\n",
    "    load_dataframe_via_trino_task.after(create_blackboard)\n",
    "\n",
    "    preprocess_dataset_task = preprocess_dataset_comp(\n",
    "        dataframe=load_dataframe_via_trino_task.outputs[\"dataframe\"]\n",
    "    )\n",
    "\n",
    "    monitor_training_task = monitor_training_comp().after(preprocess_dataset_task)\n",
    "\n",
    "    # InputPath and OutputPath like \"prep_dataset_dir\" & \"model_dir\":\n",
    "    # Use name of parameters of train component on right-hand side.\n",
    "    train_parameters = {\n",
    "        \"train_dataset_dir\": \"train_dataset_dir\",\n",
    "        \"test_dataset_dir\": \"validation_dataset_dir\",\n",
    "        \"model_dir\": \"model_dir\",\n",
    "    }\n",
    "\n",
    "    train_model_task = train_model_comp(\n",
    "        preprocess_dataset_task.outputs[\"train_dataset_dir\"],\n",
    "        preprocess_dataset_task.outputs[\"test_dataset_dir\"],\n",
    "        train_specification,\n",
    "        train_parameters,\n",
    "        model_name=model_name,\n",
    "        gpus=training_gpus,\n",
    "        tensorboard_s3_address=monitor_training_task.outputs[\"tensorboard_s3_address\"],\n",
    "        cluster_configuration_secret=cluster_configuration_secret,\n",
    "    )\n",
    "\n",
    "    convert_model_to_onnx_task = convert_model_to_onnx_comp(\n",
    "        train_model_task.outputs[\"model_dir\"]\n",
    "    )\n",
    "\n",
    "    upload_model_task = upload_model_comp(\n",
    "        file_dir=convert_model_to_onnx_task.outputs[\"onnx_model_dir\"],\n",
    "        model_name=model_name,\n",
    "    )\n",
    "\n",
    "    deploy_model_with_kserve_task = deploy_model_with_kserve_comp(model_name=model_name)\n",
    "    deploy_model_with_kserve_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f5cb70a-1e40-438f-918f-85dc903909ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/6f189cc3-98f2-4594-bfb2-c9688a1ec5c3\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/4e5e90f3-4ea3-4867-b445-83cb3c1dc281\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=4e5e90f3-4ea3-4867-b445-83cb3c1dc281)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def disable_cache_transformer(op):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf = PipelineConf()\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "pipeline_conf.data_passing_method = data_passing_methods.KubernetesVolume(\n",
    "    volume=V1Volume(\n",
    "        name=ARGUMENTS[\"blackboard\"],\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "            \"{{workflow.name}}-%s\" % ARGUMENTS[\"blackboard\"]\n",
    "        ),\n",
    "    ),\n",
    "    path_prefix=f'{ARGUMENTS[\"blackboard\"]}/',\n",
    ")\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    fraud_pipeline,\n",
    "    arguments=ARGUMENTS,\n",
    "    namespace=NAMESPACE,\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778778e2-cc0a-4f16-8300-2533a4c3e379",
   "metadata": {},
   "source": [
    "## 4.) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb28e983-a0fe-4e36-9397-ca2b849fee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'fraud-detection',\n",
       " 'versions': ['1'],\n",
       " 'platform': 'onnxruntime_onnx',\n",
       " 'inputs': [{'name': 'input_1', 'datatype': 'FP32', 'shape': [-1, 7, 103]}],\n",
       " 'outputs': [{'name': 'dense', 'datatype': 'FP32', 'shape': [-1, 1]}]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOST = f\"{MODEL_NAME}-predictor-default.{NAMESPACE}\"\n",
    "HEADERS = {\"Host\": HOST}\n",
    "MODEL_ENDPOINT = f\"http://{MODEL_NAME}-predictor-default/v2/models/{MODEL_NAME}\"\n",
    "\n",
    "res = requests.get(MODEL_ENDPOINT, headers=HEADERS)\n",
    "response = json.loads(res.text)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af216a33-cb22-47dc-9349-a229d2ee8b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 20 rows\n"
     ]
    }
   ],
   "source": [
    "def get_data_table():\n",
    "    import pandas as pd\n",
    "    from trino.dbapi import Connection\n",
    "\n",
    "    with Connection(\n",
    "        host=\"trino.trino\",\n",
    "        port=\"8080\",\n",
    "        user=\"anybody\",\n",
    "        catalog=\"postgresql\",\n",
    "        schema=\"public\",\n",
    "    ) as conn:\n",
    "        link = conn.cursor()\n",
    "        link.execute(\"SELECT * FROM transactions OFFSET 1247592\")\n",
    "        return pd.DataFrame(link.fetchall())\n",
    "\n",
    "\n",
    "vdf = get_data_table()\n",
    "print(f\"Retrieved {len(vdf)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3884d7f4-fcba-4dc9-a47e-8ca932b3f6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([14, 7, 111])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "decf5d46-5e7e-4306-8eff-0421b3026296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': \"unexpected shape for input 'input_1' for model 'fraud-detection'. Expected [-1,7,103], got [1,7,111]\"}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(res\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[0;32m---> 26\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_y\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) vs. Prediction (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(pred, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(pred, \u001b[38;5;241m0\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'outputs'"
     ]
    }
   ],
   "source": [
    "x, y = vdf.drop([0], axis=1).to_numpy(), vdf[0].to_numpy().reshape(len(vdf), 1)\n",
    "dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x, y, sequence_length=7, batch_size=128\n",
    ")\n",
    "\n",
    "HOST = f\"{MODEL_NAME}-predictor-default.{NAMESPACE}\"\n",
    "HEADERS = {\"Host\": HOST}\n",
    "PREDICT_ENDPOINT = f\"http://{MODEL_NAME}-predictor-default/v2/models/{MODEL_NAME}/infer\"\n",
    "\n",
    "for batch in dataset.take(10):\n",
    "    input_d, output_d = batch[0], batch[1]\n",
    "    for in_x, out_y in zip(input_d, output_d):\n",
    "        payload = {\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"input_1\",\n",
    "                    \"shape\": [1, 7, 111],\n",
    "                    \"datatype\": \"FP32\",\n",
    "                    \"data\": in_x.numpy().tolist(),\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        res = post(PREDICT_ENDPOINT, headers=HEADERS, data=json.dumps(payload))\n",
    "        response = json.loads(res.text)\n",
    "        print(response)\n",
    "        pred = response[\"outputs\"][0][\"data\"][0]\n",
    "        print(\n",
    "            f\"Actual ({out_y.numpy()[0]}) vs. Prediction ({round(pred, 3)} => {int(round(pred, 0))})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8aa2a0-518e-4c7e-b004-b7a5379b5dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
